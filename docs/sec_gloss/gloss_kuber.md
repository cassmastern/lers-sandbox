# Glossary of Kubernetes Terms 

| **Term** | Description | Extra Details |
|----------------------------------| --------------|---------------|
| **Annotation** | A key-value pair that is used to attach arbitrary non-identifying metadata to objects. The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by labels. Clients such as tools and libraries can retrieve this metadata. | Unlike labels, annotations are not used for selection/filtering. Common uses: build info, contact details, tool configurations. No size limit unlike labels (63 chars). |
| **API Group** | A set of related paths in Kubernetes API. You can enable or disable each API group by changing the configuration of your API server. You can also disable or enable paths to specific resources. API group makes it easier to extend the Kubernetes API. The API group is specified in a REST path and in the apiVersion field of a serialized object. | Core groups: apps/v1, networking.k8s.io/v1, rbac.authorization.k8s.io/v1. Custom resources use custom groups. Empty group is "core" (v1). |
| **API server** | Also known as `kube-apiserver`. The API server is a component of the Kubernetes control plane that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane. The main implementation of a Kubernetes API server is kube-apiserver. kube-apiserver is designed to scale horizontally—that is, it scales by deploying more instances. You can run several instances of kube-apiserver and balance traffic between those instances. | Default port: 6443 (HTTPS). Validates and configures API objects. Stores data in etcd. Supports authentication, authorization, admission control. | | Applications | The layer where various containerized applications run. The layer where various containerized applications run. | Top layer of Kubernetes architecture. Includes web apps, databases, microservices, batch jobs. Managed via Deployments, StatefulSets, Jobs, etc. |
| **cgroup (control group)** | A group of Linux processes with optional resource isolation, accounting and limits. cgroup is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network) for a collection of processes. | Foundation for container resource limits in Kubernetes. V1 (legacy) vs V2 (unified hierarchy). Manages CPU, memory, I/O bandwidth quotas. |
| **Cluster** | A set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node. The worker node(s) host the Pods that are the components of the application workload. The control plane manages the worker nodes and the Pods in the cluster. In production environments, the control plane usually runs across multiple computers and a cluster usually runs multiple nodes, providing fault-tolerance and high availability. | Minimum viable cluster: 1 control plane node + 1 worker node. Production typically 3+ control plane nodes for HA. Max ~5000 nodes per cluster. |
| **Container** | A lightweight and portable executable image that contains software and all of its dependencies. Containers decouple applications from underlying host infrastructure to make deployment easier in different cloud or OS environments, and for easier scaling. | Uses Linux namespaces and cgroups. Shares kernel with host. Different from VMs. OCI (Open Container Initiative) standard format. |
| **Container Environment Variables** | Container environment variables are name=value pairs that provide useful information into containers running in a pod. Container environment variables provide information that is required by the running containerized applications along with information about important resources to the containers. For example, file system details, information about the container itself, and other cluster resources such as service endpoints. | Can be set via: env, envFrom, ConfigMaps, Secrets. Auto-injected: Pod IP, Service endpoints. Downward API exposes Pod/Node info. |
| **Container Runtime** | The container runtime is the software that is responsible for running containers. Kubernetes supports several container runtimes: Docker, containerd, CRI-O, and any implementation of the Kubernetes CRI (Container Runtime Interface). For more information, see the CRI API and specifications. | Docker deprecated in K8s 1.24+. containerd and CRI-O are popular. Must implement CRI (Container Runtime Interface). Low-level vs high-level runtimes. |
| **Control Plane** | The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers. This layer is composed by many different components, such as (but not restricted to): <>. These components can be run as traditional operating system services (daemons) or as containers. The hosts running these components were historically called masters. | Components: kube-apiserver, etcd, kube-scheduler, kube-controller-manager. Formerly called "master nodes". Can be HA (3+ nodes). |
| **Controller** | In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state. Controllers watch the shared state of your cluster through the apiserver (part of the Control Plane). Some controllers also run inside the control plane, providing control loops that are core to Kubernetes' operations. For example: the deployment controller, the daemonset controller, the namespace controller, and the persistent volume controller (and others) all run within the kube-controller-manager. | Implements reconciliation loops. Built-in: Deployment, ReplicaSet, Node, Service controllers. Custom controllers via operators/CRDs. |
| **CustomResourceDefinition** | Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server. Custom Resource Definitions let you extend the Kubernetes API for your environment if the publicly supported API resources can't meet your needs. | CRDs enable custom resources. Used with operators for complex apps. Supports validation, versioning, subresources. Alternative to aggregated APIs. |
| **DaemonSet** | Ensures a copy of a Pod is running across a set of nodes in a cluster. Used to deploy system daemons such as log collectors and monitoring agents that typically must run on every Node. | Common use: kube-proxy, CNI agents, log/monitoring agents. One Pod per node (by default). Survives node updates with rolling updates. |
| **Data Plane** | The layer that provides capacity such as CPU, memory, network, and storage so that the containers can run and connect to a network. The layer that provides capacity such as CPU, memory, network, and storage so that the containers can run and connect to a network. | Also called "Node Plane". Consists of worker nodes running kubelet, kube-proxy, container runtime. Where actual workloads execute. |
| **Deployment** | An API object that manages a replicated application, typically by running Pods with no local state. Each replica is represented by a Pod, and the Pods are distributed among the nodes of a cluster. For workloads that do require local state, consider using a StatefulSet. | Most common workload type. Manages ReplicaSets. Supports rolling updates, rollbacks. Declarative updates. Best for stateless apps. |
| **Device Plugin** | Device plugins run on worker Nodes and provide Pods with access to resources, such as local hardware, that require vendor-specific initialization or setup steps. Device plugins advertise resources to the kubelet, so that workload Pods can access hardware features that relate to the Node where that Pod is running. You can deploy a device plugin as a DaemonSet, or install the device plugin software directly on each target Node. See Device Plugins for more information. | Examples: GPU (nvidia.com/gpu), FPGA, InfiniBand. Implements device plugin API. Requires privileged containers typically. |
| **Disruption** | Disruptions are events that lead to one or more Pods going out of service. A disruption has consequences for workload resources, such as Deployment, that rely on the affected Pods. If you, as cluster operator, destroy a Pod that belongs to an application, Kubernetes terms that a voluntary disruption. If a Pod goes offline because of a Node failure, or an outage affecting a wider failure zone, Kubernetes terms that an involuntary disruption. See Disruptions for more information. | PodDisruptionBudgets help manage voluntary disruptions. Involuntary: node crashes, network partitions. Voluntary: upgrades, scaling. |
| **Docker** | Docker (specifically, Docker Engine) is a software technology providing operating-system-level virtualization also known as containers. Docker uses the resource isolation features of the Linux kernel such as cgroups and kernel namespaces, and a union-capable file system such as OverlayFS and others to allow independent containers to run within a single Linux instance, avoiding the overhead of starting and maintaining virtual machines (VMs). | Deprecated as K8s runtime in 1.24+. Still used for image building. Dockershim removed. Use containerd or CRI-O instead. |
| **Ephemeral Container** | A Container type that you can temporarily run inside a Pod. If you want to investigate a Pod that's running with problems, you can add an ephemeral container to that Pod and carry out diagnostics. Ephemeral containers have no resource or scheduling guarantees, and you should not use them to run any part of the workload itself. | Added via kubectl debug. Share PID/network namespaces with target container. Useful for debugging minimal/distroless images. |
| **Extensions** | Extensions are software components that extend and deeply integrate with Kubernetes to support new types of hardware. Most cluster administrators will use a hosted or distribution instance of Kubernetes. As a result, most Kubernetes users will need to install extensions and fewer will need to author new ones. | Include: CNI plugins, CSI drivers, device plugins, admission controllers, custom schedulers. Extend core functionality. |
| **Image** | Stored instance of a Container that holds a set of software needed to run an application. A way of packaging software that allows it to be stored in a container registry, pulled to a local system, and run as an application. Meta data is included in the image that can indicate what executable to run, who built it, and other information. | OCI format. Immutable layers. Stored in registries (Docker Hub, ECR, GCR). Uses content-addressable storage (SHA256 hashes). |
| **Init Container** | One or more initialization containers that must run to completion before any app containers run. Initialization (init) containers are like regular app containers, with one difference: init containers must run to completion before any app containers can start. Init containers run in series: each init container must run to completion before the next init container begins. | Common uses: database migration, config setup, waiting for dependencies. Failure restarts entire Pod. Share volumes with main containers. |
| **Job** | A finite or batch task that runs to completion. Creates one or more Pod objects and ensures that a specified number of them successfully terminate. As Pods successfully complete, the Job tracks the successful completions. | For one-time tasks. CronJob for scheduled tasks. Supports parallelism, completions, backoff limits. Pods not cleaned up automatically. |
| **kube-controller-manager** | Control Plane component that runs controller processes. Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process. | Runs ~20+ built-in controllers. Leader election for HA. Watches API server for changes. Default port: 10257. |
| **kube-proxy** | kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept. kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster. kube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, kube-proxy forwards the traffic itself. | Runs on every node. Implements Service networking. Modes: iptables, ipvs, kernelspace. Usually deployed as DaemonSet. |
| **Kubectl** | A command line tool for communicating with a Kubernetes API server. You can use kubectl to create, inspect, update, and delete Kubernetes objects. | Primary CLI tool. Uses kubeconfig for authentication. Supports plugins. Commands: get, describe, create, apply, delete, logs, exec. |
| **Kubelet** | An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod. The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by Kubernetes. | Primary node agent. Communicates with API server. Manages Pod lifecycle. Health checks, resource monitoring. Default port: 10250. |
| **Kubernetes API** | The application that serves Kubernetes functionality through a RESTful interface and stores the state of the cluster. Kubernetes resources and records of intent are all stored as API objects and modified via RESTful calls to the API. The API allows configuration to be managed in a declarative way. Users can interact with the Kubernetes API directly or via tools like kubectl. The core Kubernetes API is flexible and can also be extended to support custom resources. | RESTful API. OpenAPI v3 spec. Versioned (v1, v1beta1, v1alpha1). Group/Version/Kind (GVK) structure. Authentication, authorization, admission. |
| **Label** | Tags objects with identifying attributes that are meaningful and relevant to users. Labels are key/value pairs that are attached to objects such as Pods. They are used to organize and to select subsets of objects. | Used by selectors for filtering. Max 63 chars per key/value. Common: app, version, environment. Format: prefix/name=value. |
| **LimitRange** | Provides constraints to limit resource consumption per Containers or Pods in a namespace. LimitRange limits the quantity of objects that can be created by type, as well as the amount of compute resources that may be requested/consumed by individual Containers or Pods in a namespace. | Per-namespace resource limits. Sets default/min/max CPU/memory. Prevents resource abuse. Complements ResourceQuotas. |
| **Logging** | Logs are the list of events that are logged by cluster or application. Application and systems logs can help you understand what is happening inside your cluster. The logs are particularly useful for debugging problems and monitoring cluster activity. | kubectl logs for Pod logs. Cluster-level logging needs external tools (ELK, Fluentd, Loki). Log rotation managed by container runtime. |
| **Manifest** | Specification of a Kubernetes API object in JSON or YAML format. A manifest specifies the desired state of an object that Kubernetes will maintain when you apply the manifest. Each configuration file can contain multiple manifests. | YAML preferred over JSON. Contains apiVersion, kind, metadata, spec. Can use --- to separate multiple objects in one file. |
| **Master** | Legacy term, used as synonym for nodes hosting the control plane. The term is still being used by some provisioning tools, such as kubeadm, and managed services, to label nodes with kubernetes.io/role and control placement of control plane pods. | Deprecated term. Use "control plane" instead. May still see in older docs or tools. Taint: node-role.kubernetes.io/control-plane. |
| **Minikube** | A tool for running Kubernetes locally. Minikube runs a single-node cluster inside a VM on your computer. You can use Minikube to try Kubernetes in a learning environment. | Popular for local development. Alternatives: kind, k3s, Docker Desktop. Supports multiple drivers (VirtualBox, Docker, etc.). |
| **Mirror Pod** | A pod object that a kubelet uses to represent a static pod. When the kubelet finds a static pod in its configuration, it automatically tries to create a Pod object on the Kubernetes API server for it. This means that the pod will be visible on the API server, but cannot be controlled from there. (For example, removing a mirror pod will not stop the kubelet daemon from running it). | Read-only representation of static Pods. Managed entirely by kubelet. Cannot be deleted via API server. Useful for control plane components. |
| **Name** | A client-provided string that refers to an object in a resource URL, such as /api/v1/pods/some-name. Only one object of a given kind can have a given name at a time. However, if you delete the object, you can make a new object with the same name. | Unique within namespace and resource type. DNS-1123 subdomain format. Max 253 chars. Used in URLs and references. |
| **Namespace** | An abstraction used by Kubernetes to support multiple virtual clusters on the same physical cluster. Namespaces are used to organize objects in a cluster and provide a way to divide cluster resources. Names of resources need to be unique within a namespace, but not across namespaces. | Default namespaces: default, kube-system, kube-public, kube-node-lease. Used for multi-tenancy, RBAC scope, resource quotas. |
| **Node** | A node is a worker machine in Kubernetes. A worker node may be a VM or physical machine depending on the cluster. It has local daemons or services necessary to run Pods and is managed by the control plane. The daemons on a node include kubelet kube-proxy and a container runtime implementing the CRI such as Docker. In early Kubernetes versions Nodes were called Minions. | Nodes can be managed using kubectl commands like `kubectl get nodes`, `kubectl describe node <name>`. Node capacity includes CPU, memory, pods, and storage. Nodes have conditions (Ready, OutOfDisk, MemoryPressure, etc.) that indicate their health status. |
| **Object** | An entity in the Kubernetes system. The Kubernetes API uses these entities to represent the state of your cluster. A Kubernetes object is typically a "record of intent"—once you create the object, the Kubernetes control plane works constantly to ensure that the item it represents actually exists. By creating an object, you're effectively telling the Kubernetes system what you want that part of your cluster's workload to look like; this is your cluster's desired state. | Objects have spec (desired state) and status (actual state) fields. Common objects include Pod, Service, Deployment, ConfigMap, Secret. Objects are defined in YAML/JSON manifests and managed via kubectl or API calls. |
| **Pod** | The smallest and simplest Kubernetes object. A Pod represents a set of running containers on your cluster. A Pod is typically set up to run a single primary container. It can also run optional sidecar containers that add supplementary features like logging. Pods are commonly managed by a Deployment. | Pods share networking (IP address and port space) and storage volumes. They're ephemeral - when a Pod dies, it's replaced with a new one with a different IP. Pods can have multiple containers that communicate via localhost. |
| **Pod Lifecycle** | The sequence of states through which a Pod passes during its lifetime. The Pod Lifecycle is defined by the states or phases of a Pod. There are five possible Pod phases: Pending, Running, Succeeded, Failed, and Unknown. A high-level description of the Pod state is summarized in the PodStatus phase field. | Pod phases: Pending (scheduled but containers not yet running), Running (at least one container is running), Succeeded (all containers terminated successfully), Failed (containers terminated with failure), Unknown (Pod state cannot be determined). |
| **Pod Security Policy** | Enables fine-grained authorization of Pod creation and updates. A cluster-level resource that controls security sensitive aspects of the Pod specification. The PodSecurityPolicy objects define a set of conditions that a Pod must run with in order to be accepted into the system, as well as defaults for the related fields. Pod Security Policy control is implemented as an optional admission controller. | Deprecated in Kubernetes v1.21, removed in v1.25. Replaced by Pod Security Standards (Pod Security admission controller). PSPs controlled privileged containers, host networking, volume types, user/group IDs, and security contexts. |
| **QoS Class** | QoS Class (Quality of Service Class) provides a way for Kubernetes to classify Pods within the cluster into several classes and make decisions about scheduling and eviction. QoS Class of a Pod is set at creation time based on its compute resources requests and limits settings. QoS classes are used to make decisions about Pods scheduling and eviction. Kubernetes can assign one of the following QoS classes to a Pod: Guaranteed, Burstable or BestEffort. | Guaranteed: requests = limits for all containers. Burstable: has requests/limits but not equal, or only some containers have them. BestEffort: no requests or limits set. Eviction priority: BestEffort → Burstable → Guaranteed. |
| **RBAC (Role-Based Access Control)** | Manages authorization decisions, allowing admins to dynamically configure access policies through the Kubernetes API. RBAC utilizes roles, which contain permission rules, and role bindings, which grant the permissions defined in a role to a set of users. | Four main objects: Role (namespace-scoped), ClusterRole (cluster-scoped), RoleBinding (binds Role to subjects in namespace), ClusterRoleBinding (binds ClusterRole to subjects cluster-wide). Subjects can be users, groups, or ServiceAccounts. |
| **ReplicaSet** | A ReplicaSet (aims to) maintain a set of replica Pods running at any given time. Workload objects such as Deployment make use of ReplicaSets to ensure that the configured number of Pods are running in your cluster, based on the spec of that ReplicaSet. | ReplicaSets are rarely used directly - Deployments manage ReplicaSets automatically. Uses selector to identify Pods it manages. Supports set-based selectors (in, notin, exists). Creates/deletes Pods to maintain desired replica count. |
| **Resource Quotas** | Provides constraints that limit aggregate resource consumption per Namespace. Limits the quantity of objects that can be created in a namespace by type, as well as the total amount of compute resources that may be consumed by resources in that project. | Can limit compute resources (CPU, memory), storage resources (persistent volume claims), object counts (pods, services, secrets, etc.). Enforced by admission controller. Prevents resource starvation and ensures fair sharing. |
| **Selector** | Allows users to filter a list of resources based on labels. Selectors are applied when querying lists of resources to filter them by labels. | Two types: equality-based (=, ==, !=) and set-based (in, notin, exists). Used by Services to select Pods, Deployments to select ReplicaSets, etc. Example: `environment=production,tier!=frontend`. |
| **Service** | An abstract way to expose an application running on a set of Pods as a network service. The set of Pods targeted by a Service is (usually) determined by a selector. If more Pods are added or removed, the set of Pods matching the selector will change. The Service makes sure that network traffic can be directed to the current set of Pods for the workload. | Four types: ClusterIP (internal), NodePort (external access via node ports), LoadBalancer (cloud provider load balancer), ExternalName (DNS CNAME). Services provide stable IP and DNS name for dynamic Pod sets. |
| **ServiceAccount** | Provides an identity for processes that run in a Pod. When processes inside Pods access the cluster, they are authenticated by the API server as a particular service account, for example, default. When you create a Pod, if you do not specify a service account, it is automatically assigned the default service account in the same Namespace. | ServiceAccounts can be bound to Roles/ClusterRoles via RoleBindings/ClusterRoleBindings. Tokens are automatically mounted in Pods. Can be used for RBAC, pulling images from private registries, and API access authentication. |
| **shuffle sharding** | A technique for assigning requests to queues that provides better isolation than hashing modulo the number of queues. A frequent concern is insulating different flows of requests from each other, so that a high-intensity flow does not crowd out low-intensity flows. A simple way to put requests into queues is to hash some characteristics of the request, modulo the number of queues, to get the index of the queue to use. The hash function uses as input characteristics of the request that align with flows. For example, in the Internet this is often the 5-tuple of source and destination address, protocol, and source and destination port. That simple hash-based scheme has the property that any high-intensity flow will crowd out all the low-intensity flows that hash to the same queue. Providing good insulation for a large number of flows requires a large number of queues, which is problematic. Shuffle sharding is a more nimble technique that can do a better job of insulating the low-intensity flows from the high-intensity flows. The terminology of shuffle sharding uses the metaphor of dealing a hand from a deck of cards; each queue is a metaphorical card. The shuffle sharding technique starts with hashing the flow-identifying characteristics of the request, to produce a hash value with dozens or more of bits. Then the hash value is used as a source of entropy to shuffle the deck and deal a hand of cards (queues). All the dealt queues are examined, and the request is put into one of the examined queues with the shortest length. With a modest hand size, it does not cost much to examine all the dealt cards and a given low-intensity flow has a good chance to dodge the effects of a given high-intensity flow. With a large hand size it is expensive to examine the dealt queues and more difficult for the low-intensity flows to dodge the collective effects of a set of high-intensity flows. Thus, the hand size should be chosen judiciously. | Used in Kubernetes API server's Priority and Fairness feature to ensure fair request handling across different users/groups. Helps prevent one client from overwhelming the API server and affecting other clients' requests. |
  | Manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods. Like a Deployment, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of their Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling. If you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. Although individual Pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods that replace any that have failed. | Pods have predictable names (name-0, name-1, etc.), ordered deployment/scaling, stable network identity, and persistent storage. Used for databases, distributed systems requiring stable identities. Supports rolling updates and parallel deployment policies. |
| **Static Pod** | A pod managed directly by the kubelet daemon on a specific node, without the API server observing it. | Defined by placing YAML files in kubelet's configured static pod directory (usually /etc/kubernetes/manifests). Used for control plane components (kube-apiserver, etcd, etc.). Kubelet creates mirror pods in API server for visibility. |
| **Taint** | A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of Pods on nodes or node groups. Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node. A node should only schedule a Pod with the matching tolerations for the configured taints. | Three effects: NoSchedule (prevent scheduling), PreferNoSchedule (avoid scheduling if possible), NoExecute (evict running pods). Common use cases: dedicated nodes, nodes with special hardware, nodes under maintenance. |
| **Toleration** | A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have matching taints. Tolerations and taints work together to ensure that pods are not scheduled onto inappropriate nodes. One or more tolerations are applied to a pod. A toleration indicates that the pod is allowed (but not required) to be scheduled on nodes or node groups with matching taints. | Tolerations can be exact matches or use operators (Equal, Exists). Can specify tolerationSeconds for NoExecute effect to delay eviction. System adds default tolerations for common node conditions (NotReady, Unreachable, etc.). |
| **UID** | A Kubernetes systems-generated string to uniquely identify objects. Every object created over the whole lifetime of a Kubernetes cluster has a distinct UID. It is intended to distinguish between historical occurrences of similar entities. | UIDs are generated by the API server and are immutable. Format is typically a UUID (e.g., 12345678-1234-5678-9012-123456789012). Different from names - multiple objects can have same name over time but each gets unique UID. |
| **Volume** | A directory containing data, accessible to the containers in a Pod. A Kubernetes volume lives as long as the Pod that encloses it. Consequently, a volume outlives any containers that run within the Pod, and data in the volume is preserved across container restarts. See storage for more information. | Many volume types: emptyDir (ephemeral), hostPath (node filesystem), persistentVolumeClaim (persistent storage), configMap, secret, downwardAPI, etc. Volumes are mounted into containers at specified paths. |
| **Workload** | A workload is an application running on Kubernetes. Various core objects that represent different types or parts of a workload include the DaemonSet, Deployment, Job, ReplicaSet, and StatefulSet objects. For example, a workload that has a web server and a database might run the database in one StatefulSet and the web server in a Deployment. | Workload types: Deployment (stateless apps), StatefulSet (stateful apps), DaemonSet (node agents), Job (run-to-completion tasks), CronJob (scheduled tasks). May span multiple namespaces and consist of multiple Kubernetes objects. |